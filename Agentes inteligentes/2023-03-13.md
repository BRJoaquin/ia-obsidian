https://aulas.ort.edu.uy/pluginfile.php/560514/mod_page/content/5/Clase%201-%20Intro%20y%20Bandidos%20de%20K%20Brazos.pdf?time=1678661280153

- Test de Turing como Primera definicion 
- Darthmouth Summer Research Project on Artificial Intelligence
- Que es un agente: es un entidad que interactura con un ambiente 
- Cuando podemos decir que un agente es inteligente?
-  Esta materia podria llamarse: Agentes inteligentes que aprenden
  
![[DeepinScreenshot_select-area_20230313201154.png]]


- Termino que hay que tener claros:
	- **Agente**
	- **Ambiente**
	- **Accion**
	- **Recomensa** <- lo que quiero optimizar
	- **Politica** (PI) define la relacion estimulo->reaccion
	- **Senial de recompensa** 
		- Funcion estocastica
	- **Funciones de valor**: valoralizar los movimientos
	- **Modelo del ambiente**: es opcional
	- **Problema de bandidos de K brazos**: un problema en general que nos ayuda en la materia
	- **Problemas estacionarios**: En el contexto de agentes inteligentes, un problema estacionario es aquel en el que las condiciones del entorno o del mundo no cambian con el tiempo o cambian de una manera predecible y regular. Esto significa que el agente puede suponer que las reglas del juego no cambian a medida que juega, lo que simplifica el proceso de toma de decisiones.

- Temario del curso
![[DeepinScreenshot_select-area_20230313203838.png]]

- Explotacion: Elegir una accion **greedy**. Llamamos accion greedy a una accion a que tiene maximo valor estimado Qt(a).
- Exploracion: Elegir otra accion


# El problema de k-bandidos armados

El Aprendizaje por Refuerzo es un tipo de aprendizaje automático en el que un agente aprende a comportarse en un entorno realizando acciones y observando las recompensas o penalizaciones que resultan de esas acciones.

El problema del K-bandit armado es un ejemplo clásico en el aprendizaje por refuerzo. Imagina que estás en un casino con K máquinas tragamonedas. Cada máquina tragamonedas tiene una tasa de pago diferente y tu objetivo es maximizar tus ganancias en un número fijo de rondas.

En cada ronda, debes elegir qué máquina tragamonedas jugar y recibirás una recompensa basada en la tasa de pago de la máquina elegida. Las tasas de pago son desconocidas para ti, por lo que debes explorar las máquinas jugándolas y aprendiendo sus tasas de pago.

Hay dos estrategias básicas para resolver el problema del K-bandit armado: **explotación** y **exploración**.

## Explotacion vs exploracion

En el contexto del problema del K-bandit armado en el aprendizaje por refuerzo, la explotación y la exploración son dos estrategias básicas para maximizar las ganancias.

La **explotación** consiste en elegir la máquina tragamonedas con la tasa de pago más alta en función de la información que se ha recopilado hasta ese momento. En otras palabras, el agente toma la mejor decisión posible en función de lo que ya sabe.

Por otro lado, la **exploración** implica seleccionar una máquina tragamonedas al azar o con una probabilidad determinada, incluso si la información disponible sugiere que hay otras máquinas con tasas de pago más altas. En otras palabras, el agente sacrifica la ganancia inmediata para recopilar información adicional y mejorar su toma de decisiones a largo plazo.

En general, la exploración es necesaria al comienzo del proceso de aprendizaje para recopilar suficiente información sobre las máquinas tragamonedas y luego pasar a la explotación una vez que se haya obtenido suficiente información para tomar decisiones informadas. La clave es encontrar el equilibrio adecuado entre exploración y explotación para maximizar las ganancias a largo plazo.

### Accion Greedy (explotación)

La acción "**greedy**" (o codiciosa, en español) es una estrategia de selección de acciones en el aprendizaje por refuerzo que se basa en elegir la acción que se espera que proporcione la mayor recompensa inmediata.

En otras palabras, cuando un agente utiliza la estrategia greedy, elige la acción que actualmente tiene el valor estimado más alto según la información que se ha recopilado hasta ese momento. Esta estrategia se enfoca en la **explotación** de la información disponible para maximizar la recompensa a corto plazo.

Sin embargo, **la estrategia greedy puede no ser siempre la mejor opción en el largo plazo**, especialmente al principio del proceso de aprendizaje, cuando todavía no se tiene suficiente información para tomar decisiones precisas. Es posible que se pierda la oportunidad de explorar nuevas acciones que podrían generar mayores recompensas a largo plazo. Por lo tanto, es importante equilibrar la exploración y la explotación de información para maximizar las ganancias a largo plazo.

## Actualizacion de estimaciones

![[DeepinScreenshot_select-area_20230313212256.png]]

> Dependiendo de que tanta importancia se le de a lo primeros datos a mi me podria interesar cambiar la tasaAct (que no sea 1/n)


## Metodos Upper-Confidance-Bound (UCB)

utilizado en la toma de decisiones secuenciales. Se utiliza en entornos donde un agente debe tomar decisiones en situaciones inciertas y debe equilibrar la exploración de nuevas opciones y la explotación de opciones conocidas y efectivas.

El método UCB es un enfoque de selección de acción basado en la idea de que el agente debe elegir la acción que tenga la mayor estimación de recompensa esperada, pero con un término adicional que tenga en cuenta la incertidumbre de esta estimación.

En términos simples, UCB mantiene una estimación de la recompensa esperada para cada acción posible, así como una medida de incertidumbre o variabilidad en la estimación. El algoritmo selecciona la acción con la estimación más alta de la recompensa esperada, pero con un peso adicional en función de la incertidumbre.

El peso adicional se determina mediante un parámetro de confianza, que equilibra la exploración y la explotación. A medida que el agente explora y recopila más datos, la estimación de recompensa esperada se vuelve más precisa, lo que lleva a una mayor explotación de las acciones conocidas y efectivas.

En general, el método UCB es un algoritmo de selección de acción efectivo y ampliamente utilizado en una amplia gama de aplicaciones de aprendizaje por refuerzo, como en la publicidad en línea, en la selección de noticias y en la optimización de motores de búsqueda.

## Comparacion de metodos

![[DeepinScreenshot_select-area_20230313214508.png]]

TODO:
- [ ] Documental Alphago: https://www.youtube.com/watch?v=WXuK6gekU1Y 
- [ ] ![[DeepinScreenshot_select-area_20230313213956.png]]

# Bibliografia

1) [R.S. Sutton & A.G. Barto (2018), Reinforcement Learning. An Introduction, MIT Press, 2nd ed.](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) 
2) Poole, D., Mackworth, A. (2010). Artificial Intelligence Foundations of Computational Agents. Cambridge University Press.