# Concepto de estado

...


# MDP Procesos de Decision de Markov

Los procesos de decisión de Markov (MDP, por sus siglas en inglés) son una herramienta matemática utilizada para modelar y resolver problemas de toma de decisiones secuenciales. En un MDP, un agente toma decisiones en un entorno incierto con el objetivo de maximizar una recompensa a largo plazo.

Un MDP se define formalmente como una tupla (S, A, P, R, γ), donde:

-   S es el conjunto de estados posibles del entorno.
-   A es el conjunto de acciones que el agente puede tomar.
-   P es la función de transición de estado que especifica la probabilidad de pasar de un estado a otro después de tomar una acción.
-   R es la función de recompensa que especifica la recompensa que el agente recibe por tomar una acción en un estado determinado.
-   γ es el factor de descuento que modela la importancia relativa de las recompensas a corto y largo plazo.

El proceso de toma de decisiones en un MDP se divide en ciclos de decisión, en los que el agente observa el estado actual del entorno, toma una acción y recibe una recompensa. El objetivo del agente es encontrar una política óptima, que es una función que mapea cada estado a la mejor acción que debe tomar en ese estado para maximizar la recompensa total esperada a largo plazo.

Los MDP se utilizan en una variedad de aplicaciones, incluyendo robótica, juegos, finanzas, control de procesos y planificación. La resolución de MDP implica técnicas de programación dinámica, aprendizaje por refuerzo y teoría de juegos, entre otras.

![[DeepinScreenshot_select-area_20230320193830.png]]

## Trayectoria

La trayectoria se refiere a la secuencia de estados, acciones y recompensas que se experimentan durante la ejecución de una política.

Más específicamente, una trayectoria es una secuencia de pares (s, a, r), donde s es un estado en el que se encuentra el agente, a es la acción que toma el agente en ese estado, y r es la recompensa que recibe el agente por tomar esa acción. Una trayectoria también puede denotarse como (s0, a0, r0, s1, a1, r1, ..., sn, an, rn), donde s0 es el estado inicial, ai es la acción tomada en el estado si, y ri es la recompensa recibida por tomar ai en el estado si.

En el aprendizaje por refuerzo, una política óptima se busca a partir de la observación de múltiples trayectorias y la actualización de los valores de recompensa estimados para cada estado y acción. Por lo tanto, la trayectoria es una herramienta importante para el análisis y la evaluación de las políticas en los procesos de decisión de Markov.

## Propiedad de Markov

![[DeepinScreenshot_select-area_20230320195156.png]]

La propiedad de Markov, también conocida como la propiedad de Markov de la memoria, es una propiedad fundamental de los procesos estocásticos que establece que el estado futuro de un proceso estocástico depende únicamente del estado presente y no de los estados pasados.

En otras palabras, la propiedad de Markov establece que la probabilidad de que el proceso pase a un estado futuro dado depende solo del estado actual en el que se encuentra y no de los estados anteriores que el proceso ha atravesado. Esto se conoce como la propiedad de falta de memoria o propiedad de Markov sin memoria.

La propiedad de Markov es particularmente importante en los procesos de decisión de Markov, donde los agentes toman decisiones secuenciales basadas en la observación de estados del entorno que evolucionan según un proceso estocástico. La propiedad de Markov es fundamental para la formulación y resolución de los MDP, ya que permite modelar y resolver problemas de toma de decisiones secuenciales utilizando técnicas de programación dinámica, aprendizaje por refuerzo y otros enfoques.

## Caso practico de la clase


![[DeepinScreenshot_select-area_20230320202705.png]]
Recompensa esperada para un estado-accion
![[DeepinScreenshot_select-area_20230320202615.png]]


## Retornos y episodios

En el contexto de los procesos de decisión de Markov (MDP), los retornos se refieren a la suma acumulativa de las recompensas que un agente recibe a lo largo de una trayectoria o episodio en un entorno. El retorno se utiliza comúnmente como una medida de la eficacia de una política en un MDP, donde el objetivo del agente es maximizar la recompensa total esperada a largo plazo.

Más específicamente, el retorno G_t en el tiempo t se define como la suma de las recompensas desde el tiempo t hasta el final del episodio, ponderadas por un factor de descuento γ para reflejar la importancia relativa de las recompensas a corto y largo plazo. Formalmente, se define como:

G_t = R_{t+1} + γR_{t+2} + γ^2R_{t+3} + ... + γ^{T-t-1}R_T

Donde R_t es la recompensa recibida en el tiempo t, T es el tiempo de finalización del episodio y γ es el factor de descuento (0 ≤ γ ≤ 1).

Un episodio en un MDP es una secuencia de estados, acciones y recompensas que ocurren desde un estado inicial hasta un estado final o terminal. En algunos entornos, el episodio termina cuando el agente alcanza un estado objetivo o una condición de finalización predeterminada. En otros entornos, el episodio puede tener una duración fija o continuar indefinidamente. En el aprendizaje por refuerzo, el agente explora el entorno realizando múltiples episodios y ajustando su política en función de los retornos que ha recibido.

## Tareas episodicas vs continuas

En el aprendizaje por refuerzo, los entornos se pueden clasificar en dos categorías principales: episódicos y continuos.

Los entornos episódicos son aquellos en los que el agente interactúa con el entorno en episodios discretos y aislados, donde cada episodio comienza con un estado inicial y termina en un estado final o terminal. Los episodios son independientes unos de otros, lo que significa que las acciones y las recompensas tomadas en un episodio no afectan el siguiente episodio. Ejemplos de entornos episódicos incluyen juegos de mesa como ajedrez o backgammon, o simulaciones de experimentos en ciencias.

Por otro lado, los entornos continuos son aquellos en los que el agente interactúa con el entorno de forma continua, sin un punto final definido. En este tipo de entorno, las acciones y las recompensas tomadas en un momento dado pueden afectar los estados futuros del entorno. Ejemplos de entornos continuos incluyen robots que interactúan con el mundo físico o sistemas de control de procesos.

En el aprendizaje por refuerzo, la elección de un enfoque adecuado para un entorno episódico o continuo puede influir en la elección del algoritmo de aprendizaje utilizado. En los entornos episódicos, los algoritmos de aprendizaje por refuerzo basados en modelos, como los algoritmos de programación dinámica, pueden ser efectivos. En cambio, en los entornos continuos, los algoritmos de aprendizaje por refuerzo basados en modelos, como el aprendizaje profundo o el aprendizaje por refuerzo basado en la política, pueden ser más apropiados.

## Descuento gama

En el contexto de los procesos de decisión de Markov (MDP), el descuento gamma (γ) es un parámetro que se utiliza para controlar la importancia relativa de las recompensas a corto y largo plazo. El descuento gamma se utiliza comúnmente en la definición del retorno, que es la suma acumulativa de las recompensas a lo largo de una trayectoria en un MDP.

El descuento gamma se aplica para que las recompensas a largo plazo sean consideradas menos importantes que las recompensas a corto plazo. Por lo tanto, un valor alto de gamma significa que se da mayor importancia a las recompensas a largo plazo, mientras que un valor bajo de gamma significa que se da mayor importancia a las recompensas a corto plazo.

En la práctica, el valor de gamma se selecciona según la naturaleza del problema en cuestión. Por ejemplo, si el objetivo es optimizar la rentabilidad a largo plazo de una cartera de inversión, se podría seleccionar un valor de gamma alto para que se tenga en cuenta la rentabilidad a largo plazo de los activos en la cartera. Por otro lado, si el objetivo es completar una tarea lo más rápido posible, se podría seleccionar un valor de gamma bajo para enfatizar la finalización de la tarea en el menor tiempo posible.

En general, el valor de gamma se selecciona a través de la exploración y la experimentación en el entorno, y su elección puede tener un impacto significativo en la efectividad de la política del agente en un MDP.

![[DeepinScreenshot_select-area_20230320203933.png]]

### Agente miope

Un agente miope es un tipo de agente de toma de decisiones que solo considera las recompensas inmediatas de una acción y no tiene en cuenta las consecuencias a largo plazo de esa acción.

En otras palabras, un agente miope solo considera la situación presente y no tiene en cuenta cómo sus acciones afectarán el futuro. Este enfoque puede ser efectivo en situaciones en las que las recompensas a largo plazo no son significativas o no están disponibles, pero en general, un agente miope puede perder oportunidades para maximizar su recompensa a largo plazo.

Por el contrario, un agente que considera las consecuencias a largo plazo de sus acciones se llama agente de horizonte extendido. Este tipo de agente tiene en cuenta las recompensas a corto y largo plazo al tomar decisiones, y tiene una comprensión más completa de cómo sus acciones afectarán el futuro.

En los procesos de decisión de Markov, el objetivo del agente es maximizar la recompensa total esperada a largo plazo. Por lo tanto, los agentes de horizonte extendido tienen más probabilidades de lograr este objetivo que los agentes miope. Los algoritmos de aprendizaje por refuerzo, como la programación dinámica y el aprendizaje profundo, pueden utilizarse para ayudar a los agentes a aprender políticas de toma de decisiones óptimas, ya sea en un entorno miope o de horizonte extendido.

## Funciones de valor

Las funciones de valor son una herramienta importante en el aprendizaje por refuerzo para evaluar las políticas de toma de decisiones y ayudar a los agentes a tomar mejores decisiones en un proceso de decisión de Markov (MDP).

En un MDP, una función de valor es una función que asigna un valor numérico a cada estado o estado-acción en el entorno. Las funciones de valor se utilizan para estimar la recompensa total esperada que el agente puede recibir en el futuro si toma una acción particular en un estado particular.

Hay dos tipos principales de funciones de valor:

-   La función de valor del estado (V) estima el valor esperado de estar en un estado determinado bajo una política dada. Es decir, V(s) es el valor esperado de la recompensa total que el agente puede esperar recibir si comienza en el estado s y sigue la política dada.

> V tambien se le puede llamar pi (simbolo)
  
-   La función de valor de la acción (Q) estima el valor esperado de tomar una acción determinada en un estado determinado bajo una política dada. Es decir, Q(s, a) es el valor esperado de la recompensa total que el agente puede esperar recibir si comienza en el estado s, toma la acción a y luego sigue la política dada.

En el aprendizaje por refuerzo, las funciones de valor se estiman y actualizan a través de la observación de múltiples trayectorias en el entorno y el cálculo de los retornos. Los algoritmos de aprendizaje por refuerzo, como la programación dinámica, el aprendizaje por refuerzo basado en la política y el aprendizaje profundo, se utilizan para aprender y actualizar las funciones de valor de manera eficiente, lo que ayuda al agente a tomar decisiones óptimas en un entorno MDP.

### Politica

En el aprendizaje por refuerzo, la política y las funciones de valor están estrechamente relacionadas y se utilizan juntas para guiar al agente en la toma de decisiones en un proceso de decisión de Markov (MDP).

La política en un MDP es una función que mapea cada estado a una acción que el agente debe tomar en ese estado para maximizar la recompensa total esperada a largo plazo. La política puede ser determinista o estocástica, dependiendo de si la acción seleccionada es única o se elige de una distribución de probabilidad.

Por otro lado, las funciones de valor se utilizan para evaluar la calidad de una política. La función de valor del estado (V) estima el valor esperado de estar en un estado determinado bajo una política dada, mientras que la función de valor de la acción (Q) estima el valor esperado de tomar una acción determinada en un estado determinado bajo una política dada.

En conjunto, las funciones de valor se utilizan para determinar la mejor acción que el agente debe tomar en cada estado para maximizar la recompensa total esperada a largo plazo. Esto se logra seleccionando la acción que maximiza la función de valor de la acción Q en un estado dado para una política dada, o seleccionando la acción que maximiza la función de valor del estado V en un estado dado para una política dada.

En el aprendizaje por refuerzo, las funciones de valor se estiman y actualizan a través de la observación de múltiples trayectorias en el entorno y el cálculo de los retornos. Los algoritmos de aprendizaje por refuerzo, como la programación dinámica, el aprendizaje por refuerzo basado en la política y el aprendizaje profundo, se utilizan para aprender y actualizar las funciones de valor y la política de manera eficiente, lo que ayuda al agente a tomar decisiones óptimas en un entorno MDP.

## Ecuación de Bellman

La ecuación de Bellman es una fórmula matemática fundamental en el aprendizaje por refuerzo y en la teoría de los procesos de decisión de Markov (MDP). La ecuación fue desarrollada por Richard Bellman en la década de 1950 y proporciona una forma de calcular las funciones de valor óptimas para una política dada en un MDP.

La ecuación de Bellman establece que el valor esperado de la recompensa total a largo plazo desde un estado s (o un estado-acción (s, a) en el caso de la función de valor de la acción) bajo una política dada se puede descomponer en dos partes: la recompensa inmediata R(s,a) y el valor esperado de la recompensa total en el siguiente estado s' ponderado por un factor de descuento gamma (γ), que refleja la importancia relativa de las recompensas a corto y largo plazo.

La ecuación de Bellman para la función de valor del estado (V) es:

V(s) = R(s,a) + γ * Σp(s'|s,a) * V(s')

Donde:

-   V(s) es la función de valor del estado de s
-   R(s,a) es la recompensa inmediata de estar en el estado s y tomar la acción a
-   γ es el factor de descuento (0 ≤ γ ≤ 1)
-   p(s'|s,a) es la probabilidad de transición de s a s' dado que se toma la acción a en el estado s
-   Σ es una sumatoria sobre todos los posibles estados s' alcanzables desde el estado s mediante la acción a

La ecuación de Bellman para la función de valor de la acción (Q) es:

Q(s,a) = R(s,a) + γ * Σp(s'|s,a) * max_a' Q(s',a')

Donde:

-   Q(s,a) es la función de valor de la acción de estar en el estado s y tomar la acción a
-   max_a' Q(s',a') es el valor esperado máximo de la función de valor de la acción en el estado siguiente s'

Las ecuaciones de Bellman se utilizan comúnmente en el aprendizaje por refuerzo para actualizar las estimaciones de las funciones de valor a través de la observación de múltiples trayectorias en el entorno. Los algoritmos de aprendizaje por refuerzo, como la programación dinámica, el aprendizaje por refuerzo basado en la política y el aprendizaje profundo, se utilizan para aprender y actualizar las funciones de valor y la política de manera eficiente, lo que ayuda al agente a tomar decisiones óptimas en un entorno MDP.

![[DeepinScreenshot_select-area_20230320211055.png]]

## Politica optima

La política óptima en un proceso de decisión de Markov (MDP) es la política que maximiza la recompensa total esperada a largo plazo en el entorno. En otras palabras, es la política que permite al agente tomar las mejores decisiones en cada estado para maximizar su recompensa a lo largo del tiempo.

En un MDP, la política óptima se puede obtener a través de la optimización de la función de valor óptima. La función de valor óptima es la función de valor que se obtiene al seleccionar la mejor acción en cada estado en función de la recompensa esperada a largo plazo.

En otras palabras, la función de valor óptima indica la recompensa total esperada a largo plazo que se puede obtener en cualquier estado del entorno si se sigue la política óptima. La política óptima se puede obtener a partir de la función de valor óptima seleccionando en cada estado la acción que maximiza la función de valor de la acción.

En el aprendizaje por refuerzo, la obtención de la política óptima se logra a través de la iteración de políticas y la mejora de la política en cada iteración. Los algoritmos de aprendizaje por refuerzo, como la programación dinámica, el aprendizaje por refuerzo basado en la política y el aprendizaje profundo, se utilizan para aprender y actualizar las funciones de valor y la política de manera eficiente, lo que ayuda al agente a tomar decisiones óptimas en un entorno MDP.

![[DeepinScreenshot_select-area_20230320212335.png]]


## Ecuaciones de optimalidad de Bellman

Las ecuaciones de optimalidad de Bellman son un conjunto de ecuaciones que se utilizan en el aprendizaje por refuerzo para encontrar la función de valor óptima en un proceso de decisión de Markov (MDP). Estas ecuaciones son una extensión de la ecuación de Bellman, y proporcionan una forma de calcular la función de valor óptima y la política óptima para un MDP.

Las ecuaciones de optimalidad de Bellman establecen que la función de valor óptima en un estado s es igual a la recompensa inmediata R(s,a) más el máximo valor esperado de la función de valor de la acción Q(s,a') ponderado por un factor de descuento gamma (γ), donde a' es la acción óptima que maximiza la función de valor de la acción en el siguiente estado s'.

La ecuación de optimalidad de Bellman para la función de valor del estado (V) es:

V*(s) = max_a {R(s,a) + γ * Σp(s'|s,a) * V*(s')}

Donde:

-   V*(s) es la función de valor óptima del estado de s
-   R(s,a) es la recompensa inmediata de estar en el estado s y tomar la acción a
-   γ es el factor de descuento (0 ≤ γ ≤ 1)
-   p(s'|s,a) es la probabilidad de transición de s a s' dado que se toma la acción a en el estado s
-   Σ es una sumatoria sobre todos los posibles estados s' alcanzables desde el estado s mediante la acción a
-   max_a es el operador que selecciona la acción que maximiza la expresión dentro de las llaves

La ecuación de optimalidad de Bellman para la función de valor de la acción (Q) es:

Q*(s,a) = R(s,a) + γ * Σp(s'|s,a) * max_{a'} Q*(s',a')

Donde:

-   Q*(s,a) es la función de valor óptima de la acción de estar en el estado s y tomar la acción a
-   max_{a'} es el operador que selecciona la acción que maximiza la expresión dentro de las llaves

Las ecuaciones de optimalidad de Bellman se utilizan comúnmente en el aprendizaje por refuerzo para actualizar las estimaciones de las funciones de valor a través de la observación de múltiples trayectorias en el entorno. Los algoritmos de aprendizaje por refuerzo, como la programación dinámica, el aprendizaje por refuerzo basado en la política y el aprendizaje profundo, se utilizan para aprender y actualizar las funciones de valor y la política de manera eficiente, lo que ayuda al agente a tomar decisiones óptimas en un entorno MDP.

##  Busqueda de polıticas optimas

La búsqueda de políticas óptimas es un proceso importante en el aprendizaje por refuerzo, que implica encontrar la mejor política de toma de decisiones para un proceso de decisión de Markov (MDP) dado.

Hay varios métodos para buscar políticas óptimas en un MDP. A continuación, se describen algunos de los métodos más comunes:

1.  Programación dinámica: La programación dinámica es un método de búsqueda de políticas óptimas en un MDP que implica la resolución iterativa de las ecuaciones de Bellman. Este método se utiliza para calcular la función de valor óptima y la política óptima de un MDP.

2.  Aprendizaje por refuerzo basado en la política: El aprendizaje por refuerzo basado en la política es un método de búsqueda de políticas óptimas en un MDP que implica la mejora iterativa de la política mediante la exploración del entorno y la actualización de las estimaciones de las funciones de valor. Este método se utiliza para aprender la política óptima sin necesidad de calcular la función de valor óptima.

3.  Algoritmos de gradiente: Los algoritmos de gradiente son un método de búsqueda de políticas óptimas en un MDP que implica la optimización directa de la política a través de la estimación del gradiente de la función de valor. Este método se utiliza para encontrar una política óptima sin la necesidad de calcular la función de valor.

4.  Métodos de búsqueda de políticas: Los métodos de búsqueda de políticas son un conjunto de algoritmos que buscan directamente la política óptima a través de la exploración del espacio de políticas. Estos métodos se utilizan cuando no se dispone de una función de valor o cuando la búsqueda de una función de valor es computacionalmente costosa.


En resumen, la búsqueda de políticas óptimas en un MDP es un proceso fundamental en el aprendizaje por refuerzo, y hay varios métodos disponibles para encontrar la mejor política de toma de decisiones en un entorno dado. La elección del método adecuado depende del problema específico y de las limitaciones computacionales del agente.

### Iterativo

Un algoritmo iterativo es un método de solución de un problema que implica la repetición de un conjunto de instrucciones de manera iterativa hasta que se cumpla una condición de parada. En el aprendizaje por refuerzo, los algoritmos iterativos se utilizan para encontrar la política óptima en un proceso de decisión de Markov (MDP).

El algoritmo iterativo más común utilizado en el aprendizaje por refuerzo es la programación dinámica. La programación dinámica es un método iterativo que utiliza las ecuaciones de Bellman para calcular la función de valor óptima y la política óptima en un MDP.

En la programación dinámica, se comienza por inicializar la función de valor en cero, y luego se utiliza la ecuación de Bellman para actualizar la función de valor en cada iteración. El proceso se repite hasta que la función de valor converja a la función de valor óptima. Una vez que se ha obtenido la función de valor óptima, se puede obtener la política óptima seleccionando la mejor acción en cada estado según la función de valor de la acción.

El algoritmo de iteración de políticas es otro algoritmo iterativo utilizado en el aprendizaje por refuerzo. En este algoritmo, se comienza con una política inicial aleatoria, y luego se mejora iterativamente la política a través de la evaluación y la actualización de las estimaciones de la función de valor.

En resumen, los algoritmos iterativos son un método común utilizado en el aprendizaje por refuerzo para encontrar la política óptima en un proceso de decisión de Markov. La programación dinámica y el algoritmo de iteración de políticas son dos de los algoritmos iterativos más comunes utilizados en el aprendizaje por refuerzo.

![[DeepinScreenshot_select-area_20230320220320.png]]


![[DeepinScreenshot_select-area_20230320221911.png]]

# Otros recursos

<iframe width="560" height="315" src="https://www.youtube.com/embed/B_lK-P68_Zc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>