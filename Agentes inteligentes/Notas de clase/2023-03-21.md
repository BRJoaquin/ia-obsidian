... continuacion clase anterior

# Mejora de la politica

## Policy Iteration

La iteración de políticas (Policy Iteration) es otro algoritmo utilizado para resolver Procesos de Decisión de Markov (MDP) y encontrar la política óptima que maximiza la recompensa acumulada a lo largo del tiempo. A diferencia de la iteración de valor, la iteración de políticas se enfoca en mejorar directamente la política en lugar de los valores de los estados.

Pasos del algoritmo de iteración de políticas:

1.  Inicializar una política arbitraria (π).
    
2.  Evaluar la política actual: Calcular los valores de los estados (V(s)) para la política actual utilizando la ecuación de Bellman en su forma de expectativa, resolviendo el siguiente sistema de ecuaciones lineales: V(s) = R(s, π(s)) + gamma * sum_{s'}(P(s'|s, π(s)) * V(s')) Donde:
    
    -   R(s, π(s)) es la recompensa inmediata por tomar la acción recomendada por la política actual en el estado s.
    -   gamma es el factor de descuento.
    -   P(s'|s, π(s)) es la probabilidad de transición de pasar del estado s al estado s' tomando la acción recomendada por la política actual.
    -   V(s') es el valor del estado siguiente s'.
3.  Mejorar la política: Actualizar la política seleccionando la acción que maximiza la ecuación de Bellman para cada estado: π(s) = argmax_a[R(s, a) + gamma * sum_{s'}(P(s'|s, a) * V(s'))]
    
4.  Repetir los pasos 2 y 3 hasta que la política converja (no haya cambios en la política entre iteraciones).

> V(s) = R(s, π(s)) + gamma * sum_{s'}(P(s'|s, π(s)) * V(s')) <- tener cuidado que el R podria ser una sumatoria si mi politica no es determinista

La iteración de políticas puede ser computacionalmente más eficiente que la iteración de valor en algunos casos, especialmente cuando la política óptima se alcanza antes de que los valores de los estados hayan convergido completamente. Sin embargo, el rendimiento de ambos algoritmos depende del problema específico y de sus características.
![[DeepinScreenshot_select-area_20230321202059.png]]
![[DeepinScreenshot_select-area_20230321203805.png]]
![[DeepinScreenshot_select-area_20230321204534.png]]

## Value Iteration

En el contexto de los Procesos de Decisión de Markov (MDP, por sus siglas en inglés), la iteración de valor (Value Iteration) es un algoritmo utilizado para resolver MDPs y encontrar la política óptima que maximiza la recompensa acumulada a lo largo del tiempo. Los MDPs son un modelo matemático que describe un ambiente de decisión en el que un agente toma acciones en un conjunto de estados y transiciones entre ellos, obteniendo recompensas en función de sus decisiones.

El algoritmo de iteración de valor se basa en la ecuación de Bellman, que establece que el valor de un estado es igual a la recompensa inmediata más el valor esperado de los estados futuros, descontado por un factor de descuento. La iteración de valor calcula iterativamente los valores de los estados hasta que converjan a un valor óptimo.

Pasos del algoritmo de iteración de valor:

1.  Inicializar los valores de los estados (V(s)) a cero o a valores arbitrarios.
2.  Para cada iteración, actualizar los valores de los estados utilizando la ecuación de Bellman: V(s) = max_a[R(s, a) + gamma * sum_{s'}(P(s'|s, a) * V(s'))] Donde:
    -   max_a representa la maximización sobre todas las acciones posibles (a).
    -   R(s, a) es la recompensa inmediata por tomar la acción a en el estado s.
    -   gamma es el factor de descuento, que determina la importancia de las recompensas futuras en comparación con las inmediatas.
    -   P(s'|s, a) es la probabilidad de transición de pasar del estado s al estado s' tomando la acción a.
    -   V(s') es el valor del estado siguiente s'.
3.  Repetir el paso 2 hasta que los valores de los estados converjan (cambio entre iteraciones menor que un umbral predeterminado).

Una vez que se han calculado los valores óptimos de los estados, se puede obtener la política óptima (la mejor acción para cada estado) seleccionando la acción que maximiza la ecuación de Bellman para cada estado.

![[DeepinScreenshot_select-area_20230321210246.png]]
![[DeepinScreenshot_select-area_20230321210256.png]]
![[DeepinScreenshot_select-area_20230321210439.png]]




# TODO

- [ ] Analisis Value-Itereation ![[DeepinScreenshot_select-area_20230321194236.png]]