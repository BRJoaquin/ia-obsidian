# TD(0)
****
Supongamos que tienes un agente que se mueve en una cuadr칤cula de 4x4. El objetivo del agente es llegar a la esquina inferior derecha (4,4) partiendo de la esquina superior izquierda (1,1). Las recompensas son -1 por cada movimiento. Los movimientos posibles en cada estado son abajo y derecha. 

Asuma que con una pol칤tica $\pi$ se generaron los siguientes episodios:
- **Episodio 1**: Derecha (D), Derecha (D), Derecha (D), Abajo (A), Abajo (A), Abajo (A)
- **Episodio 2**: Derecha (D), Abajo (A), Abajo (A), Derecha (D), Derecha (D), Abajo (A)
- **Episodio 3**: Abajo (A), Abajo (A), Derecha (D), Derecha (D), Derecha (D), Abajo (A)

Se pide:
- Utilize el enfoque de TD(0) para **estimar** $V_{\pi}(s)$ y $Q_{\pi}(s,a)$.
- Si utilizamos el algoritmo de SARSA con dicha pol칤tica $\pi$ fija, 쯖ambiar칤a el resultado?

> Notas:
> 	- Tasa de aprendizaje ($\alpha$): 0.1
> 	- Factor de descuento ($\gamma$): 0.9

# Ejercicio 2: Q-Learning

Utilizando los mismos episodios anteriores genere una pol칤tica $\pi_{2}$ utilizando Q-Learning.

- 쯉e puede decir que $\pi_2$ es mejor que $\pi$?
- Si se generaran miles de episodios con una pol칤tica random, 쯤u칠 ocurre con $\pi_{2}$ utilizando Q-Learning?

# Ejercicio 3: Evitando el Abismo

Una vez jugando los episodios anteriores, de repente se a침ade un "abismo" en la celda (2, 3) (utilizando la notaci칩n (fila, columna) con base en 1) y la recompensa de caer en el abismo es -10.

Se genera el siguiente episodio 游: Derecha (D), Abajo (A), Derecha (D) 

- Actualice la estimaci칩n $\pi$ utilizando TD(0).
- Actualice la pol칤tica $\pi_2$ usando Q-Learning. 

## Reflexi칩n sobre el Abismo

- **Comparaci칩n de Estrategias**: Despu칠s de completar ambas partes, reflexiona sobre c칩mo el abismo afecta las decisiones de pol칤tica en SARSA y Q-Learning. Considera c칩mo cada algoritmo se adapta a las recompensas negativas severas y qu칠 esto puede decir sobre su uso en entornos con penalizaciones significativas.

- **Riesgo vs. Seguridad**: Piensa en c칩mo la presencia del abismo puede cambiar la estrategia de exploraci칩n del agente (teniendo $\epsilon>0$). 쮼l agente se vuelve m치s cauteloso con SARSA en comparaci칩n con Q-Learning, o viceversa?

Para m치s info:
- Example 6.6: Cliff Walking (cap 6.5, Reinforcement Learning. An Introduction", R.S. Sutton & A.G. Barto (2018))
- [Temporal Difference Learning (including Q-Learning) | Reinforcement Learning Part 4](https://www.youtube.com/watch?v=AJiG3ykOxmY&pp=ygUkY2xpZmYgd2Fsa2luZyByZWluZm9yY2VtZW50IGxlYXJuaW5n)