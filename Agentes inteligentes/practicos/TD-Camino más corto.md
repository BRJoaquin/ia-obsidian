# TD(0)
****
Supongamos que tienes un agente que se mueve en una cuadr칤cula de 4x4. El objetivo del agente es llegar a la esquina inferior derecha (4,4) partiendo de la esquina superior izquierda (1,1). Las recompensas son -1 por cada movimiento. Los movimientos posibles en cada estado son abajo y derecha. 

Asuma que con una pol칤tica $\pi$ se generaron los siguientes episodios:
- **Episodio 1**: Derecha (D), Derecha (D), Derecha (D), Abajo (A), Abajo (A), Abajo (A)
- **Episodio 2**: Derecha (D), Abajo (A), Abajo (A), Derecha (D), Derecha (D), Abajo (A)
- **Episodio 3**: Abajo (A), Abajo (A), Derecha (D), Derecha (D), Derecha (D), Abajo (A)

Se pide:
- Utilize el enfoque de TD(0) para **estimar** $V_{\pi}(s)$ y $Q_{\pi}(s,a)$.
- Si utilizamos el algoritmo de SARSA con dicha pol칤tica $\pi$ fija, 쯖ambiar칤a el resultado?

> Notas:
> 	- Tasa de aprendizaje ($\alpha$): 0.1
> 	- Factor de descuento ($\gamma$): 0.9

# Ejercicio 2: Q-Learning

Utilizando los mismos episodios anteriores genere una pol칤tica $\pi_{2}$ utilizando Q-Learning.

- 쯉e puede decir que $\pi_2$ es mejor que $\pi$?

# Ejercicio 3: Evitando el Abismo

Una vez jugando los episodios anteriores, de repente se a침ade un "abismo" en la celda (2, 3) (utilizando la notaci칩n (fila, columna) con base en 1) y la recompensa de caer en el abismo es -10.

En la siguiente iteraci칩n se observa el siguiente episodio 游: Derecha (D), Abajo (A), Derecha (D) 

- Actualice la estimaci칩n $\pi$ asumiendo que se estaba ejecutando TD(0).
- Actualice la pol칤tica $\pi_2$ asumiendo que se estaba ejecutando Q-Learning. 

Para m치s info:
- Example 6.6: Cliff Walking (cap 6.5, Reinforcement Learning. An Introduction", R.S. Sutton & A.G. Barto (2018))
- [Temporal Difference Learning (including Q-Learning) | Reinforcement Learning Part 4](https://www.youtube.com/watch?v=AJiG3ykOxmY&pp=ygUkY2xpZmYgd2Fsa2luZyByZWluZm9yY2VtZW50IGxlYXJuaW5n)