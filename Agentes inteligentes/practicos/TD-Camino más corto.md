# TD(0)

Supongamos que tienes un agente que se mueve en una cuadr铆cula de 4x4. El objetivo del agente es llegar a la esquina inferior derecha (4,4) partiendo de la esquina superior izquierda (1,1). Las recompensas son -1 por cada movimiento. Los movimientos posibles en cada estado son abajo y derecha. 

Asuma que con una pol铆tica $\pi$ se generaron los siguientes episodios:
- **Episodio 1**: Derecha (D), Derecha (D), Derecha (D), Abajo (A), Abajo (A), Abajo (A)
- **Episodio 2**: Derecha (D), Abajo (A), Abajo (A), Derecha (D), Derecha (D), Abajo (A)
- **Episodio 3**: Abajo (A), Abajo (A), Derecha (D), Derecha (D), Derecha (D), Abajo (A)

Utilize el enfoque de SARSA para estimar $V_{\pi}(s)$ y $Q_{\pi}(s,a)$

> Notas:
> 	- Tasa de aprendizaje ($\alpha$): 0.1
> 	- Factor de descuento ($\gamma$): 0.9

# Ejercicio 2: Q-Learning

Utilizando los mismos episodios anteriores genere una pol铆tica $\pi_{2}$ utilizando Q-Learning.

- 驴Se puede decir que $\pi_2$ es mejor que $\pi$?
- Si se generaran miles de episodios con una pol铆tica random, 驴qu茅 ocurre con $\pi_{2}$ utilizando Q-Learning?

# Ejercicio 3: Evitando el Abismo

Una vez jugando los episodios anteriores, de repente se a帽ade un "abismo" en la celda (2, 3) (utilizando la notaci贸n (fila, columna) con base en 1) y la recompensa de caer en el abismo es -10.

Se genera el siguiente episodio : Derecha (D), Abajo (A), Derecha (D) 

- Actualice la estimaci贸n $\pi$ utilizando SARSA.
- Actualice la pol铆tica $\pi_2$ usando Q-Learning. 

## Reflexi贸n sobre el Abismo

- **Comparaci贸n de Estrategias**: Despu茅s de completar ambas partes, reflexiona sobre c贸mo el abismo afecta las decisiones de pol铆tica en SARSA y Q-Learning. Considera c贸mo cada algoritmo se adapta a las recompensas negativas severas y qu茅 esto puede decir sobre su uso en entornos con penalizaciones significativas.

- **Riesgo vs. Seguridad**: Piensa en c贸mo la presencia del abismo puede cambiar la estrategia de exploraci贸n del agente (teniendo $\epsilon>0$). 驴El agente se vuelve m谩s cauteloso con SARSA en comparaci贸n con Q-Learning, o viceversa?

Para m谩s info:
- Example 6.6: Cliff Walking (cap 6.5, Reinforcement Learning. An Introduction", R.S. Sutton & A.G. Barto (2018))
- [Temporal Difference Learning (including Q-Learning) | Reinforcement Learning Part 4](https://www.youtube.com/watch?v=AJiG3ykOxmY&pp=ygUkY2xpZmYgd2Fsa2luZyByZWluZm9yY2VtZW50IGxlYXJuaW5n)