## Definición y Contexto

Los embeddings son una técnica fundamental en el campo del [[Deep Learning|deep learning]] y el [[Procesamiento del Lenguaje Natural (NLP)|procesamiento del lenguaje natural (NLP)]]. Se utilizan para transformar datos de alta dimensionalidad y tipo categórico, como palabras o etiquetas, en vectores de baja dimensionalidad. Estos vectores contienen valores numéricos continuos. La idea principal detrás de los embeddings es representar datos complejos, como palabras, de una manera que las máquinas puedan procesar eficientemente. ver [[Vectorizacion de palabras]]

## Características Principales

### Representación Densa y Continua

- Los embeddings convierten representaciones dispersas (como one-hot encoding) en vectores densos.
- Cada dimensión del vector de embedding no tiene un significado interpretable por sí misma.
- Esta representación densa captura más información en menos dimensiones.

### Captura de Relaciones Semánticas y Contextuales

- Los embeddings son eficientes para capturar similitudes semánticas. Palabras con significados similares tienden a estar más cerca en el espacio del embedding.
- Estos modelos pueden incluso capturar relaciones y analogías complejas.
- Aprenden de los contextos en los cuales aparecen las palabras o entidades, permitiendo una representación más rica.
