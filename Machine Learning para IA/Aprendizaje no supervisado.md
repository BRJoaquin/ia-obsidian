El aprendizaje no supervisado es una categoría de técnicas de aprendizaje automático en la que los modelos se entrenan utilizando datos no etiquetados, es decir, solo con ejemplos de entradas sin las salidas correspondientes. A diferencia del aprendizaje supervisado, en el que se proporcionan respuestas correctas para guiar el proceso de aprendizaje, en el aprendizaje no supervisado, el modelo debe descubrir por sí mismo la estructura, patrones o relaciones subyacentes en los datos.

El objetivo del aprendizaje no supervisado es desarrollar modelos capaces de comprender y representar las características fundamentales de los datos sin recibir información explícita sobre cómo hacerlo. Estos modelos pueden ser útiles para analizar y explorar datos, detectar anomalías, extraer conocimientos y reducir la dimensionalidad de conjuntos de datos de alta dimensión.

Algunas técnicas y tareas comunes en el aprendizaje no supervisado incluyen:

1.  **Agrupamiento** ([[Clustering]]): El objetivo es dividir un conjunto de datos en grupos o clústeres basados en la similitud entre los ejemplos. Los algoritmos de agrupamiento, como K-means, DBSCAN y agrupamiento jerárquico, buscan identificar y agrupar puntos de datos similares sin conocer previamente las categorías.

2.  Reducción de dimensionalidad ([[Reducción de dimensionalidad]]): El propósito es reducir la cantidad de variables (dimensiones) en un conjunto de datos, manteniendo la mayor cantidad de información posible. Esto puede simplificar la representación de los datos y mejorar la eficiencia de otros algoritmos de aprendizaje automático. Ejemplos de técnicas de reducción de dimensionalidad incluyen PCA (Análisis de componentes principales), t-SNE y autoencoders.

3.  **Detección de anomalías**: La tarea consiste en identificar ejemplos que difieren significativamente del resto de los datos. Estos puntos pueden ser errores, eventos raros o comportamientos inusuales que podrían ser de interés. Algunos algoritmos de detección de anomalías incluyen Isolation Forest y LOF (Local Outlier Factor).

4.  **Aprendizaje de representaciones**: A través del aprendizaje no supervisado, los modelos pueden aprender a generar representaciones compactas y significativas de los datos de entrada. Estas representaciones pueden ser útiles para mejorar el rendimiento en tareas de aprendizaje supervisado o por refuerzo. Ejemplos de algoritmos que aprenden representaciones incluyen autoencoders y Word2Vec.