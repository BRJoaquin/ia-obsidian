El gradiente descendente es un algoritmo de optimización utilizado en [[Machine Learning]] y en redes neuronales para minimizar una función objetivo, como la [[Función de pérdida]]. La idea principal del gradiente descendente es ajustar iterativamente los parámetros del modelo en dirección opuesta al gradiente de la función de pérdida, es decir, hacia la dirección en la que la función de pérdida decrece más rápidamente, con el fin de encontrar el mínimo local o global de la función de pérdida.