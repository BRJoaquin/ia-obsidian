En el contexto del [[Machine Learning]], la varianza se refiere a la sensibilidad del modelo a pequeñas fluctuaciones en los datos de entrenamiento. Un modelo con alta varianza tiende a ser más complejo y flexible, lo que le permite ajustarse muy bien a los datos de entrenamiento, pero también puede conducir a un sobreajuste. Por otro lado, un modelo con baja varianza es más simple y generaliza mejor a partir de los datos de entrenamiento, pero puede ser menos preciso en datos no vistos si es demasiado simple y no puede capturar la complejidad de la relación subyacente entre las características y la variable objetivo.


La diferencia clave entre la [[Función de pérdida]] y la varianza es que la función de pérdida mide el rendimiento del modelo en un conjunto de datos específico, mientras que la varianza se refiere a la variabilidad del rendimiento del modelo en diferentes conjuntos de datos de entrenamiento.

![[Pasted image 20230708124715.png]]