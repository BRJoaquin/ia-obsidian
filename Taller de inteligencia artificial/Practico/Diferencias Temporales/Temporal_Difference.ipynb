{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M81eDC7bWC9y"
   },
   "source": [
    "# Métodos de Diferencias Temporales (TD)\n",
    "\n",
    "En este notebook vamos a ver métodos de diferencias temporales en particular, vamos a ver un método de control on-policy (Sarsa) y un método off-policy (Q-Learning) para estimar la funcion de valor (y la política) ótima para un problema. \n",
    "\n",
    "\n",
    "El notebook se basa en el capítulo 6 del libro de Sutton y Barto.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "Vamos a utilizar un problema clásico de Reinforcement Learning, y para ello vamos a usar otro ambiente de OpenAi gym: MountainCar: https://gym.openai.com/envs/MountainCar-v0/ o https://gymnasium.farama.org/environments/classic_control/mountain_car/.\n",
    "\n",
    "La descripcion del mismo es la siguiente:\n",
    "\n",
    "Un auto esta posicionado en un carril de una dimension entre dos montañas. El objetivo es llegar a la cima de la montaña derecha pero, el motor del auto no es lo suficientemente fuerte para hacerlo en una sola pasada (no puede simplemente acelerar y llegar). Entonces, la única forma de tener éxito es ir de atrás hacia delante repetidas veces para acumular suficiente energía para subir.\n",
    "\n",
    "![Image](https://i.ytimg.com/vi/slIJHOuTCmc/hqdefault.jpg)\n",
    "\n",
    "Este ambiente tiene representación gráfica visual en gym, para ello vamos a hacer uso de unas funciones auxliares para poder ver resultados en video en colab.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## A entregar:\n",
    "\n",
    "- Implementación de algoritmo Q-Learning\n",
    "- Implementacion de algoritmo Sarsa\n",
    "- Comparacion entre ambos para el ambiente dado, cuanto tarda cada uno en llegar al objetivo (en promedio). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZAq-yLHeHzc"
   },
   "source": [
    "### Funciones auxiliares para visualizar el ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9fNgTyITdb0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%apt-get` not found.\n"
     ]
    }
   ],
   "source": [
    "# Dependencias necesarias.\n",
    "%pip install --upgrade gymnasium > /dev/null 2>&1\n",
    "%pip install --upgrade pyvirtualdisplay > /dev/null 2>&1\n",
    "%apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ejecutar en CECOFI\n",
    "#!pip install moviepy\n",
    "#!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4dmKgVnGVXk0"
   },
   "outputs": [],
   "source": [
    "# Imports y funciones para ver el ambiente.\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import io\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import glob\n",
    "import base64\n",
    "#from gym.wrappers import Monitor\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "  \n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia5jzW5Fd-Kq"
   },
   "source": [
    "### Creación del ambiente y visualizacion de un agente aleatorio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "qSKTaNhKd-PY",
    "outputId": "0f9fd943-99d2-45b4-b229-491d937edf42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/lib/python3.9/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/k/Documents/postgrado/postgrado/Taller de inteligencia artificial/Practico/Diferencias Temporales/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/k/Documents/postgrado/postgrado/Taller de inteligencia artificial/Practico/Diferencias Temporales/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/k/Documents/postgrado/postgrado/Taller de inteligencia artificial/Practico/Diferencias Temporales/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m env\u001b[39m.\u001b[39mrender()  \u001b[39m# Queremos poder ver el ambiente. \u001b[39;00m\n\u001b[1;32m     10\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample() \n\u001b[0;32m---> 11\u001b[0m observation, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action) \n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mor\u001b[39;00m truncated:\n\u001b[1;32m     14\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gymnasium/wrappers/record_video.py:180\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_vector_env:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m terminateds \u001b[39mor\u001b[39;00m truncateds:\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose_video_recorder()\n\u001b[1;32m    181\u001b[0m \u001b[39melif\u001b[39;00m terminateds[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m truncateds[\u001b[39m0\u001b[39m]:\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose_video_recorder()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gymnasium/wrappers/record_video.py:193\u001b[0m, in \u001b[0;36mRecordVideo.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecording:\n\u001b[1;32m    192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_recorder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvideo_recorder\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m    194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecording \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorded_frames \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:161\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     clip \u001b[39m=\u001b[39m ImageSequenceClip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorded_frames, fps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes_per_sec)\n\u001b[1;32m    160\u001b[0m     moviepy_logger \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_logger \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 161\u001b[0m     clip\u001b[39m.\u001b[39;49mwrite_videofile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, logger\u001b[39m=\u001b[39;49mmoviepy_logger)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[39m# No frames captured. Set metadata.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m\u001b[39m not set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/moviepy/decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    130\u001b[0m new_a \u001b[39m=\u001b[39m [fun(arg) \u001b[39mif\u001b[39;00m (name\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m arg\n\u001b[1;32m    131\u001b[0m          \u001b[39mfor\u001b[39;00m (arg, name) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(a, names)]\n\u001b[1;32m    132\u001b[0m new_kw \u001b[39m=\u001b[39m {k: fun(v) \u001b[39mif\u001b[39;00m k\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m v\n\u001b[1;32m    133\u001b[0m          \u001b[39mfor\u001b[39;00m (k,v) \u001b[39min\u001b[39;00m k\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49mnew_a, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m clip\u001b[39m.\u001b[39mismask:\n\u001b[1;32m     21\u001b[0m     clip \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m f(clip, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m make_audio:\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[39m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[39m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[39m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m ffmpeg_write_video(\u001b[39mself\u001b[39;49m, filename, fps, codec,\n\u001b[1;32m    301\u001b[0m                    bitrate\u001b[39m=\u001b[39;49mbitrate,\n\u001b[1;32m    302\u001b[0m                    preset\u001b[39m=\u001b[39;49mpreset,\n\u001b[1;32m    303\u001b[0m                    write_logfile\u001b[39m=\u001b[39;49mwrite_logfile,\n\u001b[1;32m    304\u001b[0m                    audiofile\u001b[39m=\u001b[39;49maudiofile,\n\u001b[1;32m    305\u001b[0m                    verbose\u001b[39m=\u001b[39;49mverbose, threads\u001b[39m=\u001b[39;49mthreads,\n\u001b[1;32m    306\u001b[0m                    ffmpeg_params\u001b[39m=\u001b[39;49mffmpeg_params,\n\u001b[1;32m    307\u001b[0m                    logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m remove_temp \u001b[39mand\u001b[39;00m make_audio:\n\u001b[1;32m    310\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(audiofile):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/moviepy/video/io/ffmpeg_writer.py:213\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    211\u001b[0m     logfile \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m logger(message\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMoviepy - Writing video \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m filename)\n\u001b[0;32m--> 213\u001b[0m \u001b[39mwith\u001b[39;00m FFMPEG_VideoWriter(filename, clip\u001b[39m.\u001b[39;49msize, fps, codec \u001b[39m=\u001b[39;49m codec,\n\u001b[1;32m    214\u001b[0m                             preset\u001b[39m=\u001b[39;49mpreset, bitrate\u001b[39m=\u001b[39;49mbitrate, logfile\u001b[39m=\u001b[39;49mlogfile,\n\u001b[1;32m    215\u001b[0m                             audiofile\u001b[39m=\u001b[39;49maudiofile, threads\u001b[39m=\u001b[39;49mthreads,\n\u001b[1;32m    216\u001b[0m                             ffmpeg_params\u001b[39m=\u001b[39;49mffmpeg_params) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m    218\u001b[0m     nframes \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(clip\u001b[39m.\u001b[39mduration\u001b[39m*\u001b[39mfps)\n\u001b[1;32m    220\u001b[0m     \u001b[39mfor\u001b[39;00m t,frame \u001b[39min\u001b[39;00m clip\u001b[39m.\u001b[39miter_frames(logger\u001b[39m=\u001b[39mlogger, with_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m                                     fps\u001b[39m=\u001b[39mfps, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/moviepy/video/io/ffmpeg_writer.py:88\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.__init__\u001b[0;34m(self, filename, size, fps, codec, audiofile, preset, bitrate, withmask, logfile, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mext \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     79\u001b[0m \u001b[39m# order is important\u001b[39;00m\n\u001b[1;32m     80\u001b[0m cmd \u001b[39m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     get_setting(\u001b[39m\"\u001b[39m\u001b[39mFFMPEG_BINARY\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     82\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-y\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-loglevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m logfile \u001b[39m==\u001b[39m sp\u001b[39m.\u001b[39mPIPE \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     84\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-f\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrawvideo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     85\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-vcodec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrawvideo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-s\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (size[\u001b[39m0\u001b[39m], size[\u001b[39m1\u001b[39m]),\n\u001b[1;32m     87\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-pix_fmt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrgba\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m withmask \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mrgb24\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m---> 88\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-r\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39;49m\u001b[39m%.02f\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m fps,\n\u001b[1;32m     89\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m-an\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-i\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     90\u001b[0m ]\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m audiofile \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     cmd\u001b[39m.\u001b[39mextend([\n\u001b[1;32m     93\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m-i\u001b[39m\u001b[39m'\u001b[39m, audiofile,\n\u001b[1;32m     94\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m-acodec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcopy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     95\u001b[0m     ])\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Necesitamos \"envolver\" el ambiente para hacer uso de las funciones definidas anteriormente.\n",
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "# El resto del código es igual a lo que venimos acostumbrados.\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()  # Queremos poder ver el ambiente. \n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "        \n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXuu2iiGh-N1"
   },
   "source": [
    "### Interacción con el ambiente\n",
    "\n",
    "El ambiente cuenta con 3 acciones: Acelerar a la izquierda, frenar y acelerar a la derecha. Las recompensas son -1 por cada accion tomada con excepcion de llegar a la cima de la montaña. Un episodio se termina al alcanzar la cima o al luego de 200 interacciones.\n",
    "\n",
    "Lo que podemos observar al interactuar con el es un valor real para su Velocidad (negativa al ir para la izquierda, positiva a la derecha) y un valor real para su Posicion en la línea.\n",
    "\n",
    "Ambos valores son continuos, esto nos representa un problema, ya que queremos modelar una función de valor y una política para todos los estados posibles.\n",
    "\n",
    "Para atacar este problema vamos a **DISCRETIZAR** el ambiente, esto es: convertir la posicion y velocidad del auto en valores discretos dentro de un rango definido.\n",
    "\n",
    "Para ello vamos a definir algunas constantes y una función que nos permite obtener una representacion discreta de las observaciones del ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38xxdT-emM4u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will use 40 different values for Position and 40 for velocity (40x40 combinations)\n",
    "NUMBER_STATES = 40\n",
    "\n",
    "def discretization(env, obs):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    \n",
    "    env_den = (env_high - env_low) / NUMBER_STATES\n",
    "    pos_den = env_den[0]\n",
    "    vel_den = env_den[1]\n",
    "    \n",
    "    pos_low = env_low[0]\n",
    "    vel_low = env_low[1]\n",
    "    \n",
    "    pos_scaled = int((obs[0] - pos_low) / pos_den)\n",
    "    vel_scaled = int((obs[1] - vel_low) / vel_den)\n",
    "    \n",
    "    return pos_scaled, vel_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kMCuawcphJr"
   },
   "source": [
    "Una vez tenemos el ambiente discretizado, podemos crear una tabla que contenga el valor esperado para cada estado posible (donde en este caso, cada estado es una terna de: posicion, velocidad y accion a tomar). \n",
    "\n",
    "Esta tabla es normalmente conocida como \"Q table\" por su uso en Q learning (aunque tambien se usa, pero de manera distinta en Sarsa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jL1Gl_ovepE"
   },
   "source": [
    "### Q-Learning\n",
    "\n",
    "Vamos a comenzar implementando Q-learning para el problema actual, la implementacion corre por parte de los estudiantes.\n",
    "\n",
    "Recordamos aquí el algoritmo:\n",
    "\n",
    "![Image](https://leimao.github.io/images/blog/2019-03-14-RL-On-Policy-VS-Off-Policy/q-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T22:44:29.022297Z",
     "start_time": "2023-05-01T22:44:29.021075Z"
    },
    "id": "wiw-__ryvuoJ"
   },
   "outputs": [],
   "source": [
    "def q_learning(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "    \"\"\"\n",
    "    La función q_learning entrena un agente en un entorno utilizando Q-Learning con\n",
    "    parámetros personalizables. Devuelve la tabla Q, las recompensas por episodio,\n",
    "    los pasos por episodio y las victorias (llegar a la meta) por episodio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar la tabla Q y epsilon\n",
    "    Q_table = np.zeros((NUMBER_STATES, NUMBER_STATES, env.action_space.n))\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    # Rastreadores de estadísticas\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    wins = []\n",
    "\n",
    "    for ep_idx in tqdm_notebook(range(number_episodes)):\n",
    "        # Restablecer el entorno y obtener posición y velocidad iniciales\n",
    "        observation, info = env.reset()\n",
    "        position, velocity = discretization(env, observation)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Política epsilon-greedy\n",
    "            action = np.argmax(Q_table[position][velocity])\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "\n",
    "            # Realizar la acción y observar el resultado\n",
    "            new_observation, reward, done, truncated, _ = env.step(action)\n",
    "            new_position, new_velocity = discretization(env, new_observation)\n",
    "\n",
    "            # Regla de actualización de Q-Learning\n",
    "            Q_table[position][velocity][action] += alpha * (reward + gamma * np.max(Q_table[new_position][new_velocity]) - Q_table[position][velocity][action])\n",
    "\n",
    "            # Avanzar al siguiente estado\n",
    "            position, velocity = new_position, new_velocity\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(rewards)\n",
    "        episode_steps.append(steps)\n",
    "\n",
    "        # Contar el número de \"victorias\" (llegar a la meta) en el entorno del mountain car\n",
    "        if done:\n",
    "            wins.append(1)\n",
    "        else:\n",
    "            wins.append(0)\n",
    "\n",
    "        # Actualizar epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    return Q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "U4iL6nS5t2I5",
    "outputId": "13fdcf75-1a17-4954-81dc-70b3081f361d"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 4000\n",
    "q_table, rewards, steps, wins = q_learning(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "45FgL2C3TRql",
    "outputId": "3562b607-2a48-4752-8368-48eafef79fa2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCCsW_iJIfH"
   },
   "source": [
    "## Visualizacion del agente\n",
    "\n",
    "Vamos a ejecutar una política completamente greedy para observar lo aprendido por el algoritmo, usando la q_table retornada por el mismo para decidir la mejor accion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "rXDjSNfPt2M5",
    "outputId": "a35db66f-f3f3-42ee-d145-90ade034d9f9"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfYj49u7eseK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18GiSLEeKKtc"
   },
   "source": [
    "### Implementación de Sarsa\n",
    "\n",
    "\n",
    "![Image](https://miro.medium.com/max/2612/1*Wim9wr-jYJtZyZ_4ZJRHNg.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T23:08:19.994177Z",
     "start_time": "2023-05-01T23:08:19.952771Z"
    },
    "id": "EeopCSvcUtt_"
   },
   "outputs": [],
   "source": [
    "def sarsa(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "\n",
    "    # Inicializar la tabla Q y epsilon\n",
    "    Q_table = np.zeros((NUMBER_STATES, NUMBER_STATES, env.action_space.n))\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    # Rastreadores de estadísticas\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    wins = []\n",
    "\n",
    "    for ep_idx in tqdm_notebook(range(number_episodes)):\n",
    "        # Restablecer el entorno y obtener posición y velocidad iniciales\n",
    "        observation, info = env.reset()\n",
    "        position, velocity = discretization(env, observation)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        # Política epsilon-greedy\n",
    "        action = np.argmax(Q_table[position][velocity])\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Realizar la acción y observar el resultado\n",
    "            new_observation, reward, done, truncated, _ = env.step(action)\n",
    "            new_position, new_velocity = discretization(env, new_observation)\n",
    "\n",
    "            # Política epsilon-greedy\n",
    "            new_action = np.argmax(Q_table[new_position][new_velocity])\n",
    "            if np.random.uniform() < epsilon:\n",
    "                new_action = np.random.choice(env.action_space.n)\n",
    "\n",
    "            # Regla de actualización de Sarsa\n",
    "            Q_table[position][velocity][action] += alpha * (reward + gamma * Q_table[new_position][new_velocity][new_action] - Q_table[position][velocity][action])\n",
    "\n",
    "            # Avanzar al siguiente estado\n",
    "            position, velocity = new_position, new_velocity\n",
    "            action = new_action\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(rewards)\n",
    "        episode_steps.append(steps)\n",
    "\n",
    "        # Contar el número de \"victorias\" (llegar a la meta) en el entorno del mountain car\n",
    "        if done:\n",
    "            wins.append(1)\n",
    "        else:\n",
    "            wins.append(0)\n",
    "\n",
    "        # Actualizar epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    return Q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z0raz4aUUuI7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m NUMBER_EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m      3\u001b[0m q_table, rewards, steps, wins \u001b[38;5;241m=\u001b[39m sarsa(env, number_episodes\u001b[38;5;241m=\u001b[39mNUMBER_EPISODES, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 400\n",
    "q_table, rewards, steps, wins = sarsa(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "xhTXraYchNoY",
    "outputId": "d253b713-67c8-49c4-f58c-6f10fd517184"
   },
   "outputs": [],
   "source": [
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "pH4zGMaIhNqe",
    "outputId": "a01e976e-ee64-49f0-aa1d-de84774721e0"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
