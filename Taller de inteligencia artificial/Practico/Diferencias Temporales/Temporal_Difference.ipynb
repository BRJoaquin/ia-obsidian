{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M81eDC7bWC9y"
   },
   "source": [
    "# Métodos de Diferencias Temporales (TD)\n",
    "\n",
    "En este notebook vamos a ver métodos de diferencias temporales en particular, vamos a ver un método de control on-policy (Sarsa) y un método off-policy (Q-Learning) para estimar la funcion de valor (y la política) ótima para un problema. \n",
    "\n",
    "\n",
    "El notebook se basa en el capítulo 6 del libro de Sutton y Barto.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "Vamos a utilizar un problema clásico de Reinforcement Learning, y para ello vamos a usar otro ambiente de OpenAi gym: MountainCar: https://gym.openai.com/envs/MountainCar-v0/ o https://gymnasium.farama.org/environments/classic_control/mountain_car/.\n",
    "\n",
    "La descripcion del mismo es la siguiente:\n",
    "\n",
    "Un auto esta posicionado en un carril de una dimension entre dos montañas. El objetivo es llegar a la cima de la montaña derecha pero, el motor del auto no es lo suficientemente fuerte para hacerlo en una sola pasada (no puede simplemente acelerar y llegar). Entonces, la única forma de tener éxito es ir de atrás hacia delante repetidas veces para acumular suficiente energía para subir.\n",
    "\n",
    "![Image](https://i.ytimg.com/vi/slIJHOuTCmc/hqdefault.jpg)\n",
    "\n",
    "Este ambiente tiene representación gráfica visual en gym, para ello vamos a hacer uso de unas funciones auxliares para poder ver resultados en video en colab.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## A entregar:\n",
    "\n",
    "- Implementación de algoritmo Q-Learning\n",
    "- Implementacion de algoritmo Sarsa\n",
    "- Comparacion entre ambos para el ambiente dado, cuanto tarda cada uno en llegar al objetivo (en promedio). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZAq-yLHeHzc"
   },
   "source": [
    "### Funciones auxiliares para visualizar el ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1383680906.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [9]\u001B[0;36m\u001B[0m\n\u001B[0;31m    apt-get install xvfb\u001B[0m\n\u001B[0m            ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9fNgTyITdb0e",
    "ExecuteTime": {
     "start_time": "2023-04-18T20:15:33.966053Z",
     "end_time": "2023-04-18T20:15:35.626021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencias necesarias.\n",
    "!pip install --upgrade gymnasium > /dev/null 2>&1\n",
    "!pip install --upgrade pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T20:15:36.698332Z",
     "end_time": "2023-04-18T20:15:37.479949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /home/k/miniconda3/lib/python3.9/site-packages (1.0.3)\r\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (2.27.0)\r\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (0.1.10)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (1.23.1)\r\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (0.4.8)\r\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (4.4.2)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (4.64.0)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/k/miniconda3/lib/python3.9/site-packages (from moviepy) (2.28.1)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/k/miniconda3/lib/python3.9/site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/k/miniconda3/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/k/miniconda3/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/k/miniconda3/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.9.24)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/k/miniconda3/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.11)\r\n",
      "zsh:1: no matches found: gymnasium[classic-control]\r\n"
     ]
    }
   ],
   "source": [
    "# Para ejecutar en CECOFI\n",
    "!pip install moviepy\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4dmKgVnGVXk0"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Xvfb'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyvirtualdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Display\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m display \u001B[38;5;28;01mas\u001B[39;00m ipythondisplay\n\u001B[0;32m---> 15\u001B[0m display \u001B[38;5;241m=\u001B[39m \u001B[43mDisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvisible\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1400\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m900\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m display\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow_video\u001B[39m():\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/pyvirtualdisplay/display.py:54\u001B[0m, in \u001B[0;36mDisplay.__init__\u001B[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcls\u001B[39m:\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown backend: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolor_depth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolor_depth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbgcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbgcolor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_xauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_xauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# check_startup=check_startup,\u001B[39;49;00m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanage_global_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanage_global_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py:44\u001B[0m, in \u001B[0;36mXvfbDisplay.__init__\u001B[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fbdir \u001B[38;5;241m=\u001B[39m fbdir\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dpi \u001B[38;5;241m=\u001B[39m dpi\n\u001B[0;32m---> 44\u001B[0m \u001B[43mAbstractDisplay\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mPROGRAM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_xauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_xauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanage_global_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanage_global_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/pyvirtualdisplay/abstractdisplay.py:85\u001B[0m, in \u001B[0;36mAbstractDisplay.__init__\u001B[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pipe_wfd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retries_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 85\u001B[0m helptext \u001B[38;5;241m=\u001B[39m \u001B[43mget_helptext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprogram\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_displayfd \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-displayfd\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m helptext\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_displayfd:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/pyvirtualdisplay/util.py:13\u001B[0m, in \u001B[0;36mget_helptext\u001B[0;34m(program)\u001B[0m\n\u001B[1;32m      6\u001B[0m cmd \u001B[38;5;241m=\u001B[39m [program, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-help\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# py3.7+\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# stderr = p.stderr\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# py3.6 also\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstderr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m _, stderr \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mcommunicate()\n\u001B[1;32m     21\u001B[0m helptext \u001B[38;5;241m=\u001B[39m stderr\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/subprocess.py:951\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001B[0m\n\u001B[1;32m    947\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[1;32m    948\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[1;32m    949\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m--> 951\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    952\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/subprocess.py:1821\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001B[0m\n\u001B[1;32m   1819\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errno_num \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1820\u001B[0m         err_msg \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mstrerror(errno_num)\n\u001B[0;32m-> 1821\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001B[1;32m   1822\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(err_msg)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Xvfb'"
     ]
    }
   ],
   "source": [
    "# Imports y funciones para ver el ambiente.\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import io\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import glob\n",
    "import base64\n",
    "#from gym.wrappers import Monitor\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "  \n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia5jzW5Fd-Kq"
   },
   "source": [
    "### Creación del ambiente y visualizacion de un agente aleatorio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "qSKTaNhKd-PY",
    "outputId": "0f9fd943-99d2-45b4-b229-491d937edf42"
   },
   "outputs": [],
   "source": [
    "# Necesitamos \"envolver\" el ambiente para hacer uso de las funciones definidas anteriormente.\n",
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "# El resto del código es igual a lo que venimos acostumbrados.\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()  # Queremos poder ver el ambiente. \n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "        \n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXuu2iiGh-N1"
   },
   "source": [
    "### Interacción con el ambiente\n",
    "\n",
    "El ambiente cuenta con 3 acciones: Acelerar a la izquierda, frenar y acelerar a la derecha. Las recompensas son -1 por cada accion tomada con excepcion de llegar a la cima de la montaña. Un episodio se termina al alcanzar la cima o al luego de 200 interacciones.\n",
    "\n",
    "Lo que podemos observar al interactuar con el es un valor real para su Velocidad (negativa al ir para la izquierda, positiva a la derecha) y un valor real para su Posicion en la línea.\n",
    "\n",
    "Ambos valores son continuos, esto nos representa un problema, ya que queremos modelar una función de valor y una política para todos los estados posibles.\n",
    "\n",
    "Para atacar este problema vamos a **DISCRETIZAR** el ambiente, esto es: convertir la posicion y velocidad del auto en valores discretos dentro de un rango definido.\n",
    "\n",
    "Para ello vamos a definir algunas constantes y una función que nos permite obtener una representacion discreta de las observaciones del ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38xxdT-emM4u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will use 40 different values for Position and 40 for velocity (40x40 combinations)\n",
    "NUMBER_STATES = 40\n",
    "\n",
    "def discretization(env, obs):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    \n",
    "    env_den = (env_high - env_low) / NUMBER_STATES\n",
    "    pos_den = env_den[0]\n",
    "    vel_den = env_den[1]\n",
    "    \n",
    "    pos_low = env_low[0]\n",
    "    vel_low = env_low[1]\n",
    "    \n",
    "    pos_scaled = int((obs[0] - pos_low) / pos_den)\n",
    "    vel_scaled = int((obs[1] - vel_low) / vel_den)\n",
    "    \n",
    "    return pos_scaled, vel_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kMCuawcphJr"
   },
   "source": [
    "Una vez tenemos el ambiente discretizado, podemos crear una tabla que contenga el valor esperado para cada estado posible (donde en este caso, cada estado es una terna de: posicion, velocidad y accion a tomar). \n",
    "\n",
    "Esta tabla es normalmente conocida como \"Q table\" por su uso en Q learning (aunque tambien se usa, pero de manera distinta en Sarsa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jL1Gl_ovepE"
   },
   "source": [
    "### Q-Learning\n",
    "\n",
    "Vamos a comenzar implementando Q-learning para el problema actual, la implementacion corre por parte de los estudiantes.\n",
    "\n",
    "Recordamos aquí el algoritmo:\n",
    "\n",
    "![Image](https://leimao.github.io/images/blog/2019-03-14-RL-On-Policy-VS-Off-Policy/q-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiw-__ryvuoJ"
   },
   "outputs": [],
   "source": [
    "def q_learning(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "\n",
    "  # Initialize Q_table as a matrix of NumberStatesxNumberStatesxNumberActions\n",
    "  \n",
    "  # Initialize epsilon\n",
    "  epsilon = epsilon_start\n",
    "  \n",
    "  # Stat trackers\n",
    "  # Track the accumulated reward per episode\n",
    "  # Track the number of steps taken in each episode\n",
    "  wins = []              # Track the number of times we reach the goal (1 if we do, 0 otherwise)\n",
    "\n",
    "  for ep_idx in tqdm_notebook(range(number_episodes)):\n",
    "\n",
    "    # Reset the environment and get initial position and velocity\n",
    "\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not (done or truncated):\n",
    "\n",
    "      # Epsilon greedy policy.\n",
    "      action = np.argmax(q_table[position][velocity])\n",
    "      if np.random.uniform() < epsilon:\n",
    "        action = np.random.choice(env.action_space.n)\n",
    "      \n",
    "      # Take action on env and observe result.\n",
    "\n",
    "      # Q-Learning Update rule.\n",
    "      \n",
    "      # Move the state forward.\n",
    "    \n",
    "    # Count number of \"wins\" for mountain car env.\n",
    "    if done:\n",
    "      wins.append(1)\n",
    "    else:\n",
    "      wins.append(0)\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "  return q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "U4iL6nS5t2I5",
    "outputId": "13fdcf75-1a17-4954-81dc-70b3081f361d"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 4000\n",
    "q_table, rewards, steps, wins = q_learning(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "45FgL2C3TRql",
    "outputId": "3562b607-2a48-4752-8368-48eafef79fa2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCCsW_iJIfH"
   },
   "source": [
    "## Visualizacion del agente\n",
    "\n",
    "Vamos a ejecutar una política completamente greedy para observar lo aprendido por el algoritmo, usando la q_table retornada por el mismo para decidir la mejor accion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "rXDjSNfPt2M5",
    "outputId": "a35db66f-f3f3-42ee-d145-90ade034d9f9"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfYj49u7eseK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18GiSLEeKKtc"
   },
   "source": [
    "### Implementación de Sarsa\n",
    "\n",
    "\n",
    "![Image](https://miro.medium.com/max/2612/1*Wim9wr-jYJtZyZ_4ZJRHNg.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeopCSvcUtt_"
   },
   "outputs": [],
   "source": [
    "def sarsa(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "\n",
    "  ## Your code here\n",
    "\n",
    "  return q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0raz4aUUuI7"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 4000\n",
    "q_table, rewards, steps, wins = sarsa(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "xhTXraYchNoY",
    "outputId": "d253b713-67c8-49c4-f58c-6f10fd517184"
   },
   "outputs": [],
   "source": [
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "pH4zGMaIhNqe",
    "outputId": "a01e976e-ee64-49f0-aa1d-de84774721e0"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
