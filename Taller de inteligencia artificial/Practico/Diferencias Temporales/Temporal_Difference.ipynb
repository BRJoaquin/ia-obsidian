{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M81eDC7bWC9y"
   },
   "source": [
    "# Métodos de Diferencias Temporales (TD)\n",
    "\n",
    "En este notebook vamos a ver métodos de diferencias temporales en particular, vamos a ver un método de control on-policy (Sarsa) y un método off-policy (Q-Learning) para estimar la funcion de valor (y la política) ótima para un problema. \n",
    "\n",
    "\n",
    "El notebook se basa en el capítulo 6 del libro de Sutton y Barto.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## Ambiente\n",
    "\n",
    "Vamos a utilizar un problema clásico de Reinforcement Learning, y para ello vamos a usar otro ambiente de OpenAi gym: MountainCar: https://gym.openai.com/envs/MountainCar-v0/ o https://gymnasium.farama.org/environments/classic_control/mountain_car/.\n",
    "\n",
    "La descripcion del mismo es la siguiente:\n",
    "\n",
    "Un auto esta posicionado en un carril de una dimension entre dos montañas. El objetivo es llegar a la cima de la montaña derecha pero, el motor del auto no es lo suficientemente fuerte para hacerlo en una sola pasada (no puede simplemente acelerar y llegar). Entonces, la única forma de tener éxito es ir de atrás hacia delante repetidas veces para acumular suficiente energía para subir.\n",
    "\n",
    "![Image](https://i.ytimg.com/vi/slIJHOuTCmc/hqdefault.jpg)\n",
    "\n",
    "Este ambiente tiene representación gráfica visual en gym, para ello vamos a hacer uso de unas funciones auxliares para poder ver resultados en video en colab.\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "## A entregar:\n",
    "\n",
    "- Implementación de algoritmo Q-Learning\n",
    "- Implementacion de algoritmo Sarsa\n",
    "- Comparacion entre ambos para el ambiente dado, cuanto tarda cada uno en llegar al objetivo (en promedio). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZAq-yLHeHzc"
   },
   "source": [
    "### Funciones auxiliares para visualizar el ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fNgTyITdb0e"
   },
   "outputs": [],
   "source": [
    "# Dependencias necesarias.\n",
    "!pip install --upgrade gymnasium > /dev/null 2>&1\n",
    "!pip install --upgrade pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ejecutar en CECOFI\n",
    "#!pip install moviepy\n",
    "#!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dmKgVnGVXk0"
   },
   "outputs": [],
   "source": [
    "# Imports y funciones para ver el ambiente.\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import io\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "import glob\n",
    "import base64\n",
    "#from gym.wrappers import Monitor\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "  \n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia5jzW5Fd-Kq"
   },
   "source": [
    "### Creación del ambiente y visualizacion de un agente aleatorio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "qSKTaNhKd-PY",
    "outputId": "0f9fd943-99d2-45b4-b229-491d937edf42"
   },
   "outputs": [],
   "source": [
    "# Necesitamos \"envolver\" el ambiente para hacer uso de las funciones definidas anteriormente.\n",
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "# El resto del código es igual a lo que venimos acostumbrados.\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()  # Queremos poder ver el ambiente. \n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "        \n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXuu2iiGh-N1"
   },
   "source": [
    "### Interacción con el ambiente\n",
    "\n",
    "El ambiente cuenta con 3 acciones: Acelerar a la izquierda, frenar y acelerar a la derecha. Las recompensas son -1 por cada accion tomada con excepcion de llegar a la cima de la montaña. Un episodio se termina al alcanzar la cima o al luego de 200 interacciones.\n",
    "\n",
    "Lo que podemos observar al interactuar con el es un valor real para su Velocidad (negativa al ir para la izquierda, positiva a la derecha) y un valor real para su Posicion en la línea.\n",
    "\n",
    "Ambos valores son continuos, esto nos representa un problema, ya que queremos modelar una función de valor y una política para todos los estados posibles.\n",
    "\n",
    "Para atacar este problema vamos a **DISCRETIZAR** el ambiente, esto es: convertir la posicion y velocidad del auto en valores discretos dentro de un rango definido.\n",
    "\n",
    "Para ello vamos a definir algunas constantes y una función que nos permite obtener una representacion discreta de las observaciones del ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38xxdT-emM4u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will use 40 different values for Position and 40 for velocity (40x40 combinations)\n",
    "NUMBER_STATES = 40\n",
    "\n",
    "def discretization(env, obs):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    \n",
    "    env_den = (env_high - env_low) / NUMBER_STATES\n",
    "    pos_den = env_den[0]\n",
    "    vel_den = env_den[1]\n",
    "    \n",
    "    pos_low = env_low[0]\n",
    "    vel_low = env_low[1]\n",
    "    \n",
    "    pos_scaled = int((obs[0] - pos_low) / pos_den)\n",
    "    vel_scaled = int((obs[1] - vel_low) / vel_den)\n",
    "    \n",
    "    return pos_scaled, vel_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kMCuawcphJr"
   },
   "source": [
    "Una vez tenemos el ambiente discretizado, podemos crear una tabla que contenga el valor esperado para cada estado posible (donde en este caso, cada estado es una terna de: posicion, velocidad y accion a tomar). \n",
    "\n",
    "Esta tabla es normalmente conocida como \"Q table\" por su uso en Q learning (aunque tambien se usa, pero de manera distinta en Sarsa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jL1Gl_ovepE"
   },
   "source": [
    "### Q-Learning\n",
    "\n",
    "Vamos a comenzar implementando Q-learning para el problema actual, la implementacion corre por parte de los estudiantes.\n",
    "\n",
    "Recordamos aquí el algoritmo:\n",
    "\n",
    "![Image](https://leimao.github.io/images/blog/2019-03-14-RL-On-Policy-VS-Off-Policy/q-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wiw-__ryvuoJ",
    "ExecuteTime": {
     "start_time": "2023-05-01T22:44:29.021075Z",
     "end_time": "2023-05-01T22:44:29.022297Z"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "    \"\"\"\n",
    "    La función q_learning entrena un agente en un entorno utilizando Q-Learning con\n",
    "    parámetros personalizables. Devuelve la tabla Q, las recompensas por episodio,\n",
    "    los pasos por episodio y las victorias (llegar a la meta) por episodio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar la tabla Q y epsilon\n",
    "    Q_table = np.zeros((NUMBER_STATES, NUMBER_STATES, env.action_space.n))\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    # Rastreadores de estadísticas\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    wins = []\n",
    "\n",
    "    for ep_idx in tqdm_notebook(range(number_episodes)):\n",
    "        # Restablecer el entorno y obtener posición y velocidad iniciales\n",
    "        observation, info = env.reset()\n",
    "        position, velocity = discretization(env, observation)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Política epsilon-greedy\n",
    "            action = np.argmax(Q_table[position][velocity])\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "\n",
    "            # Realizar la acción y observar el resultado\n",
    "            new_observation, reward, done, truncated, _ = env.step(action)\n",
    "            new_position, new_velocity = discretization(env, new_observation)\n",
    "\n",
    "            # Regla de actualización de Q-Learning\n",
    "            Q_table[position][velocity][action] += alpha * (reward + gamma * np.max(Q_table[new_position][new_velocity]) - Q_table[position][velocity][action])\n",
    "\n",
    "            # Avanzar al siguiente estado\n",
    "            position, velocity = new_position, new_velocity\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(rewards)\n",
    "        episode_steps.append(steps)\n",
    "\n",
    "        # Contar el número de \"victorias\" (llegar a la meta) en el entorno del mountain car\n",
    "        if done:\n",
    "            wins.append(1)\n",
    "        else:\n",
    "            wins.append(0)\n",
    "\n",
    "        # Actualizar epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    return Q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "U4iL6nS5t2I5",
    "outputId": "13fdcf75-1a17-4954-81dc-70b3081f361d"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 4000\n",
    "q_table, rewards, steps, wins = q_learning(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "45FgL2C3TRql",
    "outputId": "3562b607-2a48-4752-8368-48eafef79fa2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCCsW_iJIfH"
   },
   "source": [
    "## Visualizacion del agente\n",
    "\n",
    "Vamos a ejecutar una política completamente greedy para observar lo aprendido por el algoritmo, usando la q_table retornada por el mismo para decidir la mejor accion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "rXDjSNfPt2M5",
    "outputId": "a35db66f-f3f3-42ee-d145-90ade034d9f9"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfYj49u7eseK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18GiSLEeKKtc"
   },
   "source": [
    "### Implementación de Sarsa\n",
    "\n",
    "\n",
    "![Image](https://miro.medium.com/max/2612/1*Wim9wr-jYJtZyZ_4ZJRHNg.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EeopCSvcUtt_",
    "ExecuteTime": {
     "start_time": "2023-05-01T23:08:19.952771Z",
     "end_time": "2023-05-01T23:08:19.994177Z"
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(env, number_episodes=100, alpha=0.1, gamma=0.9, epsilon_start=0.9, epsilon_min=0.1, epsilon_decay=0.9):\n",
    "\n",
    "    # Inicializar la tabla Q y epsilon\n",
    "    Q_table = np.zeros((NUMBER_STATES, NUMBER_STATES, env.action_space.n))\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    # Rastreadores de estadísticas\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    wins = []\n",
    "\n",
    "    for ep_idx in tqdm_notebook(range(number_episodes)):\n",
    "        # Restablecer el entorno y obtener posición y velocidad iniciales\n",
    "        observation, info = env.reset()\n",
    "        position, velocity = discretization(env, observation)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "\n",
    "        # Política epsilon-greedy\n",
    "        action = np.argmax(Q_table[position][velocity])\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Realizar la acción y observar el resultado\n",
    "            new_observation, reward, done, truncated, _ = env.step(action)\n",
    "            new_position, new_velocity = discretization(env, new_observation)\n",
    "\n",
    "            # Política epsilon-greedy\n",
    "            new_action = np.argmax(Q_table[new_position][new_velocity])\n",
    "            if np.random.uniform() < epsilon:\n",
    "                new_action = np.random.choice(env.action_space.n)\n",
    "\n",
    "            # Regla de actualización de Sarsa\n",
    "            Q_table[position][velocity][action] += alpha * (reward + gamma * Q_table[new_position][new_velocity][new_action] - Q_table[position][velocity][action])\n",
    "\n",
    "            # Avanzar al siguiente estado\n",
    "            position, velocity = new_position, new_velocity\n",
    "            action = new_action\n",
    "            rewards += reward\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(rewards)\n",
    "        episode_steps.append(steps)\n",
    "\n",
    "        # Contar el número de \"victorias\" (llegar a la meta) en el entorno del mountain car\n",
    "        if done:\n",
    "            wins.append(1)\n",
    "        else:\n",
    "            wins.append(0)\n",
    "\n",
    "        # Actualizar epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    return Q_table, episode_rewards, episode_steps, wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z0raz4aUUuI7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMountainCar-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m NUMBER_EPISODES \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m400\u001B[39m\n\u001B[1;32m      3\u001B[0m q_table, rewards, steps, wins \u001B[38;5;241m=\u001B[39m sarsa(env, number_episodes\u001B[38;5;241m=\u001B[39mNUMBER_EPISODES, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, epsilon_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "NUMBER_EPISODES = 400\n",
    "q_table, rewards, steps, wins = sarsa(env, number_episodes=NUMBER_EPISODES, alpha=0.3, epsilon_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "xhTXraYchNoY",
    "outputId": "d253b713-67c8-49c4-f58c-6f10fd517184"
   },
   "outputs": [],
   "source": [
    "# Average results every N timesteps for better visualization.\n",
    "average_range = 100\n",
    "episode_ticks = int(NUMBER_EPISODES / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "avg_steps = np.array(steps).reshape((episode_ticks, average_range))\n",
    "avg_steps = np.mean(avg_steps, axis=1)\n",
    "\n",
    "avg_wins = np.array(wins).reshape((episode_ticks, average_range))\n",
    "avg_wins = np.mean(avg_wins, axis=1)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_rewards)\n",
    "plt.title(\"Episode Accumulated Reward\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_steps)\n",
    "plt.title(\"Steps needed per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number Steps\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(episode_ticks), avg_wins)\n",
    "plt.title(\"Win breakdown\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Win\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "pH4zGMaIhNqe",
    "outputId": "a01e976e-ee64-49f0-aa1d-de84774721e0"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"MountainCar-v0\",render_mode=\"rgb_array\"))\n",
    "\n",
    "observation,_ = env.reset()\n",
    "position, velocity = discretization(env, observation)\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = np.argmax(q_table[position][velocity])\n",
    "    observation, reward, done, truncated, info = env.step(action) \n",
    "    \n",
    "    position, velocity = discretization(env, observation)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
