{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "En este laboratorio vamos a explorar los métodos de Monte Carlo (Capítulo 5 del libro de Sutton y Barto). Para esto, vamos a volver a utilizar un ambiente definido en OpenAi gym (https://gym.openai.com/), es el caso esta vez de otro ambiente sencillo: el juego de Blackjack. El ambiente esta inspirado por el ejemplo 5.1 del libro, ya esta incluido en gym, por lo que no es necesario crearlo desde cero. \n",
    "\n",
    "***\n",
    "### Juego de Blackjack\n",
    "\n",
    "Es un juego de cartas donde el objetivo es obtener cartas que sumen lo más cercano a 21 posible, sin pasarnos. Jugamos contra un dealer fijo unicamente.\n",
    "\n",
    "Reglas:\n",
    "   \n",
    "- Las cartas con figuras (Jotas, Reinas y Reyes) tienen valor de 10.\n",
    "- Los Ases pueden valer 11 ó 1, cuando vale 11 se lo llama \"usable\".\n",
    "- En este caso jugamos con un mazo infinito (con reemplazo).\n",
    "- El dealer comienza con una carta boca arriba y una boca abajo.\n",
    "- El jugador puede pedir una carta (HIT) hasta que decida quedarse (STICK) o exceeda los 21 puntos (BUST).\n",
    "- Cuando el jugador de queda (STICK), el dealer muestra su carta boca abajo y pide cartas hasta que su suma sea 17 o más.\n",
    "- Si el dealer se pasa de 21, el jugador gana. En caso contrario, gana quien tenga la suma más cerca de 21.\n",
    "\n",
    "Ambiente:\n",
    "- La reward por perder es -1, por ganar es +1 y por pedir carta es 0.\n",
    "- Cada observacion es una tupla que tiene: \n",
    "    - la suma del jugador\n",
    "    - la carta boca arriba del dealer (1-10 donde 1 es un As)\n",
    "    - True o False si el jugador tiene un As usable o no\n",
    "\n",
    "***\n",
    "\n",
    "### A entregar:\n",
    "\n",
    "- Notebook con solución a los algoritmos presentados\n",
    "- Analisis de la funcion de valor estimada para distinto número de episodios a visitar (ej: 100, 1000, 10000, 50000). Se busca que puedan demostrar entendimiento del algoritmo y sus resultados (Esperamos gráficas y analisis de las mismas). \n",
    "- Estimaciones de funcion de valor para otras dos politicas definidas por el estudiante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para utilizar la nueva version de GYM, si es necesario reiniciar ambiente\n",
    "#!pip install --upgrade gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Jugador: 20, Dealer: 5, As usable: False \n",
      "Nuevo estado: (20, 5, False), Recompensa: 1.0, Estado Final: True\n"
     ]
    }
   ],
   "source": [
    "# Imports necesarios\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Creacion del ambiente\n",
    "#env = gym.make('Blackjack-v0')\n",
    "env = gym.make('Blackjack-v1',natural=False, sab=True)\n",
    "\n",
    "# Acciones: 0 = Stick, 1 = HIT\n",
    "print(env.observation_space)\n",
    "\n",
    "#obs = env.reset()\n",
    "obs, _ = env.reset()\n",
    "print(f\"Jugador: {obs[0]}, Dealer: {obs[1]}, As usable: {obs[2]} \")\n",
    "\n",
    "\n",
    "#nueva_obs, reward, done, _ = env.step(0)\n",
    "nueva_obs, reward, done, truncated, _ = env.step(0)\n",
    "\n",
    "print(f\"Nuevo estado: {nueva_obs}, Recompensa: {reward}, Estado Final: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (18, 7, True)\n",
      "state: (12, 7, False) reward: 0.0 done: False truncated: False\n",
      "state: (13, 7, False) reward: 0.0 done: False truncated: False\n",
      "state: (13, 7, False) reward: -1.0 done: False truncated: True\n",
      "Reward:-1.0\n"
     ]
    }
   ],
   "source": [
    "# Caso de prueba\n",
    "#state,_ = env.reset()\n",
    "state,_ = env.reset()\n",
    "print('state:', state)\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    #state, reward, done, _ = env.step(action)\n",
    "    state, reward, truncated, done, _ = env.step(action)\n",
    "    print('state:', state, 'reward:', reward, 'done:', done, 'truncated:', truncated)\n",
    "    if done or truncated:\n",
    "        print(f'Reward:{reward}')                \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El algoritmo que vamos a implementar\n",
    "\n",
    "Vamos a comenzar con el algoritmo 5.1 del libro: `First Visit MC prediction` para estimar la función de valor de una política arbitraria. \n",
    "\n",
    "![Image](https://marcinbogdanski.github.io/rl-sketchpad/RL_An_Introduction_2018/assets/0501_FV_MC_Pred.png)\n",
    "\n",
    "En este caso vamos a querer evaluar una politica que se queda si el jugador tiene un valor mayor o igual a 19, y pide cartas en caso contrario. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STICK = 0\n",
    "HIT = 1\n",
    "       \n",
    "## Politica\n",
    "def sample_policy(observation):\n",
    "    #TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generar Episodio\n",
    "def generate_episode(policy, env):\n",
    "    states, actions, rewards = [], [], []    \n",
    "    #TODO: Implement\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_episode(sample_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Algoritmo\n",
    "def first_visit_mc_prediction(policy, env, number_episodes, gamma=1):\n",
    "    #TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'popitem'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15236/361560569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_visit_mc_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'popitem'"
     ]
    }
   ],
   "source": [
    "# Uso copy popitem modifica la estructura\n",
    "from copy import copy\n",
    "\n",
    "value = first_visit_mc_prediction(sample_policy, env, number_episodes=50000)\n",
    "\n",
    "valueaux = copy(value)\n",
    "for i in range(10):\n",
    "    print(valueaux.popitem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def plot_value_distribution(value):\n",
    "    min_x = min(k[0] for k in value.keys())\n",
    "    max_x = max(k[0] for k in value.keys())\n",
    "    min_y = min(k[1] for k in value.keys())\n",
    "    max_y = max(k[1] for k in value.keys())\n",
    "\n",
    "    player_sum = np.arange(min_x, max_x + 1)\n",
    "    dealer_show = np.arange(min_y, max_y + 1)\n",
    "    X, Y = np.meshgrid(player_sum, dealer_show)\n",
    "\n",
    "    usable_ace = np.array([False, True])\n",
    "    state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace)))\n",
    "    for i, player in enumerate(player_sum):\n",
    "        for j, dealer in enumerate(dealer_show):\n",
    "            for k, ace in enumerate(usable_ace):\n",
    "                state_values[i, j, k] = value[player, dealer, ace]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    #ax = fig.gca(projection='3d')\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    plt.title(\"Distribucion de valores / sin As usable\")\n",
    "    plt.xlabel(\"Suma jugador\")\n",
    "    plt.ylabel(\"Suma dealer\")\n",
    "    ax.view_init(ax.elev, -120)\n",
    "    ax.plot_surface(X, Y, state_values[:, :, 0].T, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    plt.show()\n",
    "\n",
    "    #Gráfico de contorno\n",
    "    cp = plt.contourf(X, Y, state_values[:, :, 0].T, cmap=cm.coolwarm)\n",
    "    plt.title(\"Contorno distribucion de valores / sin As usable\")\n",
    "    plt.xlabel(\"Suma jugador\")\n",
    "    plt.ylabel(\"Suma dealer\")        \n",
    "    plt.colorbar(cp)\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    #ax = fig.gca(projection='3d')\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    plt.title(\"Distribucion de valores / con As usable\")\n",
    "    plt.xlabel(\"Suma jugador\")\n",
    "    plt.ylabel(\"Suma dealer\")\n",
    "    ax.view_init(ax.elev, -120)\n",
    "    ax.plot_surface(X, Y, state_values[:, :, 1].T, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    plt.show()\n",
    "    \n",
    "    #Gráfico de contorno\n",
    "    cp = plt.contourf(X, Y, state_values[:, :, 1].T, cmap=cm.coolwarm)\n",
    "    plt.title(\"Contorno distribucion de valores / con As usable\")\n",
    "    plt.xlabel(\"Suma jugador\")\n",
    "    plt.ylabel(\"Suma dealer\")        \n",
    "    plt.colorbar(cp)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional\n",
    "En esta sección presentamos las definiciones de funciones del resto de las técnicas vistas en teórico para aquellos que quieran implementarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction_Q(policy, env, number_episodes, gamma=1):\n",
    "    #TODO: Implement\n",
    "    \"\"\"\n",
    "    Los siguientes snippets pueden serle de utilidad:\n",
    "        Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        states_actions = list(zip(states, actions))\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Observa algo extraño en los valores Q?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15236/2694371237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mQvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_visit_mc_prediction_Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "Qvalues = first_visit_mc_prediction_Q(sample_policy, env, number_episodes=50000)\n",
    "Qvaluesaux = copy(Qvalues)\n",
    "for i in range(10):\n",
    "    print(Qvaluesaux.popitem())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_prediction_Q_with_initial_exploration(policy, env, number_episodes, gamma=1):\n",
    "    #TODO: Implement\n",
    "    \"\"\"\n",
    "    Los siguientes snippets pueden serle de utilidad:\n",
    "        random_action = env.action_space.sample()\n",
    "        random_state = env.observation_space.sample()\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de implementar Control MC, definiremos la clase Policy.\n",
    "\n",
    "Esta se puede instanciar tanto para implementar:\n",
    "- Una política puramente greedy (ε = 0) para Control MC **con** exploración inicial\n",
    "- Una política ε-greedy para Control MC **sin** exploración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"\n",
    "    Implements an epsilon-soft policy, one that returs a probability distribution for each possible action.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        Q: dictionary of state-action keys -> array of returns.\n",
    "        number_actions: number of possible actions in the env.\n",
    "        epsilon: exploration parameter where 1 explores all the time and 0 is greedy. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, Q, number_actions, epsilon=0.1):\n",
    "        self.Q = Q\n",
    "        self.number_actions = number_actions\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __call__(self, observation):\n",
    "        #TODO: implement\n",
    "        # Se debe retornar el vector de probabilidades (página 14 del teórico).\n",
    "        # Recordar, si ε = 0 -> la política es la greedy (página 13 del teórico).\n",
    "        pass\n",
    "    \n",
    "    def update(self, new_Q):\n",
    "        self.Q = new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control_without_initial_exploration(env, num_episodes, gamma=1, epsilon=0.1):\n",
    "    #TODO: Implement\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
